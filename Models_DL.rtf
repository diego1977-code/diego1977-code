{\rtf1\ansi\ansicpg1252\cocoartf2707
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww25700\viewh18300\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 set.seed(917);\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 half_size <- floor(0.67 * nrow(whole_final_cohort))\
\
random_sample <- sample(seq_len(nrow(whole_final_cohort)), size = half_size)\
\
train_data <- whole_final_cohort[random_sample, ]\
\
test_data <- whole_final_cohort[-random_sample, ]\
\
\
control <- trainControl(method="repeatedcv", number=10, repeats=5)\
\
\
\
#### plot relative of factors and like lasso srink to zero irrelevant variables ### \
\
summary(mdl$finalModel)\
                              \
summary(mdl$bestTune)\
\
\
### To have the coefs have to do.. ###\
train$Age=as.numeric(train$Age)\
test$Age=as.numeric(test$Age)\
\
\
\'91######## train <- train[,6:21] <- sapply(train[,6:21],as.numeric) ###### \
\
train_control<-trainControl(method="repeatedcv", number=10, repeats=5)\
\
\
gives the final coefs and pvalues  for the best tun model \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 mtry <- sqrt(ncol(train_data))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 coco <- expand.grid(.mtyr=seq(0,0.1,by=0.01))\
\
\
\
mdl = train(AVGcryst ~ Age + Sex+ Income+education+FD+mri_info_manufacturer+ DMN_VAN,, data =train_data,method="lm",\
 trControl=control, tunegrid=coco )\
 \
\
train_fit = train(AVGcryst ~ Age + Sex+ Income+education+FD+mri_info_manufacturer+ DMN_VAN, data =train_data,method="lm",\
                  trControl=control,\
                   tuneGrid = mdl$bestTune)\
 \
p<-predict(mdl,newdata=test_data)\
\
\
\
##### Person r as a indicator of accuracy ####  \
\
r= cor (p,y)\
\
### Determination coefficient to comunincate how much the model explain the outcome ##  \
\
reg1=lm(p ~y)\
 summary(reg1)\
\

\f1\b Give the resutls ( coef and p-values)for the model 
\f0\b0 \
\
\
\
\
######### CONFIRMATION WITH LASSO ############\
\
\
######## Hypermarameter tunning at alpha =1 for LASSO regularization goal is to confirm the variables are there ######  \
\
\
#### This are the lines to get the actual or precise value of lambda for alpha = 1 #### \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
 ##### mdl_lasso <- train( AVGcryst ~ Age + Sex+ Income+education+FD+ DMN,\
   data = train,\
   method = "glmnet",\
   metric = "RMSE",\
   preProcess = c("center", "scale"),\
   tuneGrid = expand.grid(\
     .alpha = 1,  \
     .lambda = seq(0, 5, length.out = 101)\
   ),\
   trControl = train_control) ###### \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
set.seed(91);\
\
half_size <- floor(0.67 * nrow(whole_final_cohort))\
\
random_sample <- sample(seq_len(nrow(whole_final_cohort)), size = half_size)\
\
train_data <- whole_final_cohort[random_sample, ]\
\
test_data <- whole_final_cohort[-random_sample, ]\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 train_y=train_data[ ,c("composite_crystilized")]\
train_x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,train_data)[, -1] #Dropping the intercept column)\
\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DMN_C <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DMN_C[-1,] \
#### Variable importance using boost gradient descendent alghoritm #### \
\
model_gbmDMNcr= gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDMNcr)\
\
\
rel_imp_DMNcr=summary.gbm(model_gbmDMNcr)\
\
 rel_imp_DMNcr=data.frame(rel_imp_DMNcr)\
\
\
train_x=model.matrix(composite_crystilized~ Age + Sex+ Income+education+FD+MRI_manufacturer+ DMN_VAN,train_data)[, -1] #Dropping the intercept column)\
\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DMN_VAN_C<- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DMN_VAN_C[-1,] \
\
model_gbmDMN_VANcr = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_VAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDMN_VANcr)\
\
 rel_imp_DMN_VANcr=summary.gbm(model_gbmDMN_VANcr)\
\
 rel_imp_DMN_VANcr=data.frame(rel_imp_DMN_VANcr)\
\
################\
train_x=model.matrix(composite_crystilized~ Age + Sex+ Income+education+FD+MRI_manufacturer+ VAN,train_data)[, -1] #Dropping the intercept column)\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_VAN_C <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_VAN_C[-1,] \
\
model_gbmVANcr = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmVANcr)\
\
rel_imp_VANcr=summary.gbm(model_gbmVANcr)\
\
 rel_imp_VANcr=data.frame(rel_imp_VANcr)\
\
\
\
\
################\
train_x=model.matrix(composite_crystilized~ Age + Sex+ Income+education+FD+MRI_manufacturer+ DAN,train_data)[, -1] #Dropping the intercept column)\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DAN_C<- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DAN_C[-1,] \
\
model_gbmDANcr= gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDANcr)\
\
\
summary(model_gbmDANcr)\
\
rel_imp_DANcr=summary.gbm(model_gbmDANcr)\
\
 rel_imp_DANcr=data.frame(rel_imp_DANcr)\
\
\
\
######################\
\
################\
train_x=model.matrix(composite_crystilized~ Age + Sex+ Income+education+FD+MRI_manufacturer+ DMN_DAN,train_data)[, -1] #Dropping the intercept column)\
\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DMN_DAN_C <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DMN_DAN_C[-1,] \
model_gbmDMN_DANcr = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_DAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDMN_DANcr)\
\
\
\
rel_imp_DMN_DANcr=summary.gbm(model_gbmDMN_DANcr)\
\
 rel_imp_DMN_DANcr=data.frame(rel_imp_DMN_DANcr)\
\
\
\
####################\
####################\
\
train_x=model.matrix(composite_crystilized~ Age + Sex+ Income+education+FD+MRI_manufacturer+ VAN_DAN,train_data)[, -1] #Dropping the intercept column)\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_VAN_DAN_C <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_VAN_DAN_C[-1,] \
\
model_gbmVAN_DANcr = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN_DAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmVAN_DANcr)\
\
\
rel_imp_VAN_DANcr=summary.gbm(model_gbmVAN_DANcr)\
\
 rel_imp_VAN_DANcr=data.frame(rel_imp_VAN_DANcr)\
\
A= rel_imp_DMNcr[,-1]\
B=rel_imp_DMN_DANcr [,-1]\
C=rel_imp_DMN_VANcr [,-1]\
D=rel_imp_VANcr [,-1]\
E=rel_imp_DANcr [,-1]\
F=rel_imp_VAN_DANc[,-1]\
\
rel_imp_cry=rbind()\
\
rel_imp_cry=rbind(A,B,C,D,E,F)\
\
colnames(rel_imp_cry)[1]  <- "Education"\
 colnames(rel_imp_cry)[3]  <- "Network_Connectivity"\
 colnames(rel_imp_cry)[4]  <- "FD"\
 colnames(rel_imp_cry)[5]  <- "Sex"\
 colnames(rel_imp_cry)[6]  <- "Income"\
 colnames(rel_imp_cry)[7]  <- "MRI_manufacturer"\
\
row.names(rel_imp_cry)[1] <-   "DMN"\
row.names(rel_imp_cry)[2] <-   "DMN_DAN"\
row.names(rel_imp_cry)[3] <-   "DMN_VAN"\
row.names(rel_imp_cry)[4] <-   "VAN"\
row.names(rel_imp_cry)[5] <-   "DAN"\
row.names(rel_imp_cry)[6] <-   "VAN DAN"\
\
\
\
library('plot.matrix')\
data(Titanic.cramer)\
par(mar=c(5.1, 4.1, 4.1, 4.1)) # adapt margins\
plot(as.assoc(el_imp_cry))\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 #################### Now for fluid intelligence ############################\
\
\
###################\
\
\
\
\
\
train_y=train_data[ ,c("composite_fluid\'94)]\
train_x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,train_data)[, -1] #Dropping the intercept column)\
\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DMN_F <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DMN_F[-1,] \
\
\
\
\
\

\f1\b #### Variable importance using boost gradient descendent alghoritm: wich is the weigth of the variables neural - socioeconimic - demographic?  #### \
\
\

\f0\b0 \
\
model_gbmDMNf = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDMNf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
summary(model_gbmDMNf)\
\
\
rel_imp_DMNf=summary.gbm(model_gbmDMNf)\
\
 rel_imp_DMNf=data.frame(rel_imp_DMNf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
train_x=model.matrix(composite_fluid~ Age + Sex+ Income+education+FD+MRI_manufacturer+ DMN_VAN,train_data)[, -1] #Dropping the intercept column)\
\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DMN_VAN_F<- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DMN_VAN_F[-1,] \
\
model_gbmDMN_VANf = gbm(composite_fluid~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_VAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDMN_VANf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
rel_imp_DMN_VANf=summary.gbm(model_gbmDMN_VANf)\
\
 rel_imp_DMN_VANf=data.frame(rel_imp_DMN_VANf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
################\
train_x=model.matrix(composite_fluid~ Age + Sex+ Income+education+FD+MRI_manufacturer+ VAN,train_data)[, -1] #Dropping the intercept column)\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_VAN_F <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_VAN_F[-1,] \
\
model_gbmVANf = gbm(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmVANf)\
\
rel_imp_VANf=summary.gbm(model_gbmVANf)\
\
 rel_imp_VANf=data.frame(rel_imp_VANf)\
################\
\
train_x=model.matrix(composite_fluid~ Age + Sex+ Income+education+FD+MRI_manufacturer+ DAN,train_data)[, -1] #Dropping the intercept column)\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DAN_F<- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DAN_F[-1,] \
\
model_gbmDANf = gbm(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDANf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
rel_imp_DANf=summary.gbm(model_gbmDANf)\
\
 rel_imp_DANf=data.frame(rel_imp_DANf)\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 ######################\
\
################\
train_x=model.matrix(composite_fluid~ Age + Sex+ Income+education+FD+MRI_manufacturer+ DMN_DAN,train_data)[, -1] #Dropping the intercept column)\
\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_DMN_DAN_F <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_DMN_DAN_F[-1,] \
\
model_gbmDMN_DANf = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_DAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmDMN_DANf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
rel_imp_DMN_DANf=summary.gbm(model_gbmDMN_DANf)\
\
 rel_imp_DMN_DANf=data.frame(rel_imp_DMN_DANf)\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
####################\
####################\
\
train_x=model.matrix(composite_fluid~ Age + Sex+ Income+education+FD+MRI_manufacturer+ VAN_DAN,train_data)[, -1] #Dropping the intercept column)\
ridge1_cv <- cv.glmnet(x = train_x, y =train_y,\
                       ## type.measure: loss to use for cross-validation.\
                       type.measure = "mse",\
                       ## K = 10 is the default.\
                       nfold = 10,\
                       ## \'91alpha = 1\'92 is the lasso penalty, and \'91alpha = 0\'92 the ridge penalty.\
                       alpha = 0)\
\
plot(ridge1_cv)\
\
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]\
alasso1_cv <- cv.glmnet(x = train_x, y = train_y,\
                        type.measure = "mse",\
                        nfold = 10,\
                        alpha = 1,\
                        penalty.factor = 1 / abs(best_ridge_coef),\
                        keep = TRUE)\
best_alasso_coef_VAN_DAN_F <- coef(alasso1_cv, s = alasso1_cv$lambda.1se)\
best_alasso_coef_VAN_DAN_F[-1,] \
\
model_gbmVAN_DANf = gbm(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN_DAN,\
                data = train_data,\
                distribution = "gaussian",\
                cv.folds = 10,\
                shrinkage = .01,\
                n.minobsinnode = 10,\
                n.trees = 1000)\
\
summary(model_gbmVAN_DANf)\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
rel_imp_VAN_DANf=summary.gbm(model_gbmVAN_DANf)\
\
 rel_imp_VAN_DANf=data.frame(rel_imp_VAN_DANf)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
AA= rel_imp_DMNf[,-1]\
BB=rel_imp_DMN_DANf [,-1]\
CC=rel_imp_DMN_VANf [,-1]\
DD=rel_imp_VANf [,-1]\
EE=rel_imp_DANf [,-1]\
FF=rel_imp_VAN_DANf[,-1]\
\
\
rel_imp_flu=rbind(AA,BB,CC,DD,EE,FF)\
\
 colnames(rel_imp_flu)[1]  <- "Education"\
 colnames(rel_imp_flu)[2]  <- \'93Age\'94\
 colnames(rel_imp_flu)[3]  <- "Network_Connectivity"\
 colnames(rel_imp_flu)[4]  <- "FD"\
 colnames(rel_imp_flu)[5]  <- "Sex"\
 colnames(rel_imp_flu)[6]  <- "Income"\
 colnames(rel_imp_flu)[7]  <- "MRI_manufacturer"\
\
row.names(rel_imp_flu)[1] <-   "DMN"\
row.names(rel_imp_flu)[2] <-   "DMN_DAN"\
row.names(rel_imp_flu)[3] <-   "DMN_VAN"\
row.names(rel_imp_flu)[4] <-   "VAN"\
row.names(rel_imp_flu)[5] <-   "DAN"\
row.names(rel_imp_flu)[6] <-   "VAN DAN"\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\

\f1\b \

\f0\b0 ############################################################## Now ridge projection for the non-zero coefs calculated before and adding to the penalization factor correction for FWE to the coefficients with Holm-Bonferroni  #################################\
\
library(hdi)\
\
\
\
###### P values of the ridge projection LASSO ########\
\
####  Crystilized intelligence ##### \
\
\
set.seed(188)\
\
train_y=train_data[ ,c("composite_crystilized")]\
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_DMN_cry=fit.ridge.scaled$pval.corr \
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_DAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_DMN_DAN_cry=fit.ridge.scaled$pval.corr \
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_VAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_DMN_VAN_cry=fit.ridge.scaled$pval.corr  \
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_DAN_cry=fit.ridge.scaled$pval.corr\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4226\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
 y=train_data[ ,c("composite_fluid\'94)]\
\
\
\
y=train_data[ ,c("composite_fluid")]\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_DMN_flu = fit.ridge.scaled$pval.corr\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ FPN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_FPN_flu = fit.ridge.scaled$pval.corr \
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_DAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_DMN_DAN_flu=fit.ridge.scaled$pval.corr\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_VAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_DMN_VAN_flu=fit.ridge.scaled$pval.corr\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ FPN_DAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_FPN_DAN_flu = fit.ridge.scaled$pval.corr\
\
\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DAN,train_data)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
\
pvalues_DAN_flu = fit.ridge.scaled$pval.corr\
\
\
### gives optimal Laaso ## # \
\
mdl_lasso$bestTune\
\
ggplot(mdl_lasso) +\
  labs(title = "Lasso Regression Parameter Tuning", x = "lambda")\
\
\
#### Gives the coefs of thre optimal model ### \
\
coef(mdl_lasso$finalModel, mdl_lasso$finalModel$lambdaOpt)\
\
\
##### Make predictions on test data and test model performance #### \
\
pred.full <- predict(mdl_lasso,newdata=test)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 pred.lasso <- as.vector(predict(mdl_lasso,test , s=mdl_lasso$finalModel$lambdaOpt))\
\
Actual_intelligence= test[ ,c(\'93AVGcryst\'94)]\
\
\
cor(Actual_intellicence,pred.lasso)\
\
\
\
reg1<-lm(pred.lasso ~Actual_intelligence )\
\
\
abline(reg1, col="blue",lwd=5)\
\
\
\
###### PLOT both accuracy of the models and variale importance FOR the Model DMN_VAN > crystilized iintelligence #### \
\
\
par(mfrow=c(1,2))\
 \
plot1=plot(Actual_intelligence,pred.lasso , xlab="Observed crystalized Intelligence" ,\
         ylab="Predicted crystilized intelligence (LASSO)",font.lab=1,cex.lab=1, cex.sub=1)\
\
abline(reg1, col="red",lwd=4)\
\
summary(model_gbm)\
\
\

\f1\b ############## ##############NOW LASSO in the whole population FWE correction  Holm- Bonferroni ###############################\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4226\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b0 \cf0 \
library(hdi)\
\
set.seed(18)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1\b \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4226\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b0 \cf0 y=whole_final_cohort[ ,c("composite_crystilized")]\
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
 fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
pvalues_FWEc_DMN_cry=fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
\
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_DMN_VAN_cry= fit.ridge.scaled$pval.corr\
\
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_DAN_cry= fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_VAN_cry= fit.ridge.scaled$pval.corr\
x=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_VAN_DAN_cry= fit.ridge.scaled$pval.corr\
 y=whole_final_cohort[ ,c("composite_fluid")]\
\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_DMN_flu=fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_fluid~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fit.ridge <- ridge.proj(x, y,multiplecorr.method = "holm")\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_DMN_DAN_flu=fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_fluid~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN_VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_DMN_VAN_flu=fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_DAN_flu=fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_VAN_flu=fit.ridge.scaled$pval.corr\
\
x=model.matrix(composite_fluid~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ VAN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
 fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "holm")\
pvalues_FWEc_VAN_DAN_flu=fit.ridge.scaled$pval.corr\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\

\f1\b ########################################### NOW ESTIMATION of beta coef by boostrap ###############
\f0\b0 ########################################################\
\
\
\
library(parallel)\
library(readxl)\
library(sjPlot)\
\
library(boot)\
library(lmboot)\
\
\
### FOR CRYSTILIZED #### \
dev.new()\
Seed=set.seed(32934)\
\
par(mfrow=c(2,6))\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 y=whole_final_cohort[ ,c("composite_crystilized")]\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
x1=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
DMNcrWildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
mean(DMNcrWildObj$bootEstParam[,12],)\
\
fig1=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DMN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
\
x1=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
\
\
fig2=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
x1=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
fig3=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DMN VAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
\
x1=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
\
WildObj$origEstParam\
\
fig4=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
x1=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
\
WildObj$origEstParam\
\
fig5=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas VAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
x1=model.matrix(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
\
WildObj$origEstParam\
\
fig6=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas VAN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\

\f1\b \

\f0\b0 \
\
\
y=whole_final_cohort[ ,c("composite_fluid\'94)]\
\
x1=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+ DMN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
\
fig7=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DMN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
x1=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
\
fig8=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
x1=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
WildObj$origEstParam\
\
fig9=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DMN VAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
\
x1=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
\
WildObj$origEstParam\
\
fig10=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
\
x1=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)\
\
WildObj$origEstParam\
\
fig11=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas VAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
x1=model.matrix(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN_DAN,whole_final_cohort)[, -1] #Dropping the intercept column)\
\
fig12=hist(WildObj$bootEstParam[,12],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression)")),\
    \
      xlab= " Betas VAN  DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)\
\
 A=quantile(WildObj$bootEstParam[,12], probs=c(.025))\
 B=quantile(WildObj$bootEstParam[,12], probs=c(.975))\
C=mean(WildObj$bootEstParam[,12],)\
abline(v = A, col="black", lwd=2, lty=1)\
abline(v = B, col="blue", lwd=2, lty=1)\
abline(v = C, col="red", lwd=3, lty=2)\
\
rug(WildObj$bootEstParam[,12], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",\
    quiet = getOption("warn") < 0)\
\
\
\
\
\

\f1\b \
################### Now the  kfold linear models cross-validated ###########################
\f0\b0 \
\
\
\
\
train_control<-trainControl(method="repeatedcv", number=5, repeats=10)\
Model1<- train(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN, data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
fold_dataDMNcr <- lapply(Model1$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
\
Model2<- train(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_DAN, data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDMN_DANcr <- lapply(Model2$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model3<- train(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_VAN, data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDMN_VANcr <- lapply(Model3$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model4<- train(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DAN, data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDANcr <- lapply(Model4$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
\
Model5<- train(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN, data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataVANcr <- lapply(Model5$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model6<- train(composite_crystilized ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN_DAN, data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDAN_VANcr <- lapply(Model6$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model7<- train(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN,data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDMNf <- lapply(Model7$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model8<- train(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_DAN,data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDMN_DANf <- lapply(Model8$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model9<- train(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DMN_VAN,data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDMN_VANf <- lapply(Model9$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model10<- train(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+DAN,data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDANf <- lapply(Model10$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model11<- train(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN,data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataVANf <- lapply(Model11$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
Model12<- train(composite_fluid ~ Age + Sex+ Income+MRI_manufacturer+ education+FD+VAN_DAN,data =train_data, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")\
\
fold_dataDAN_VANf <- lapply(Model12$control$index, function(index) train_data[index,]) %>% \
    bind_rows(.id = "Fold")\
plot1=ggplot(fold_dataDMNcr, aes(DMN,composite_crystilized,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
     geom_point(shape = 21, fill = "red3",\
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN", y = "score crystallized cogniton") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
plot2=ggplot(fold_dataDMN_DANcr, aes(DMN_DAN,composite_crystilized,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "green",\
               color = "green2", size = 3) +  theme_classic() +labs(x =  "DMN DAN", y = "score crystallized cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
plot3=ggplot(fold_dataDMN_VANcr, aes(DMN_VAN,composite_crystilized,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "green4",\
               color = "green3", size = 3) +  theme_classic() +labs(x =  "DMN VAN", y = "score crystallized cogniton")+ theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
plot4=ggplot(fold_dataVAN_DANcr, aes(VAN_DAN,composite_crystilized,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "yellow2",\
               color = "yellow4", size = 3) +  theme_classic() +labs(x =  "VAN DAN", y = "score crystallized cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
\
plot5=ggplot(fold_dataVANcr, aes(VAN,composite_crystilized,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "purple",\
               color = "purple4", size = 3) +  theme_classic() +labs(x =  "VAN", y = "score crystallized cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
\
plot6=ggplot(fold_dataDANcr, aes(DAN,composite_crystilized,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "blue",\
               color = "blue4", size = 3) +  theme_classic() +labs(x =  "DAN", y = "score crystallized cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
plot7=ggplot(fold_dataDMNf, aes(DMN,composite_fluid,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
     geom_point(shape = 21, fill = "red3",\
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN", y = "score fluid cogniton") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
\
plot8=ggplot(fold_dataDMN_DANf, aes(DMN_DAN,composite_fluid,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "green",\
               color = "green2", size = 3) +  theme_classic() +labs(x =  "DMN DAN", y = "score fluid cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
plot9=ggplot(fold_dataDMN_VANf, aes(DMN_VAN,composite_fluid,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "green4",\
               color = "green3", size = 3) +  theme_classic() +labs(x =  "DMN VAN", y = "score fluid cogniton")+ theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
plot10=ggplot(fold_dataVAN_DANf, aes(VAN_DAN,composite_fluid,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "yellow2",\
               color = "yellow4", size = 3) +  theme_classic() +labs(x =  "VAN DAN", y = "score fluid cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
\
plot11=ggplot(fold_dataVANf, aes(VAN,composite_fluid,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "purple",\
               color = "purple4", size = 3) +  theme_classic() +labs(x =  "VAN", y = "score fluid cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
\
plot12=ggplot(fold_dataDANf, aes(DAN,composite_fluid,col = Fold,fill=Fold),size=0.5, alpha=0.1) + \
      geom_point(shape = 21, fill = "blue",\
               color = "blue4", size = 3) +  theme_classic() +labs(x =  "DAN", y = "score fluid cogniton")+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.title = element_text(size = 15))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+\
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_fill_viridis_d()\
\
ggarrange(plot1,plot2,plot3,plot4,plot5,plot6,plot7,plot8,plot9,plot10,plot11,plot12, ncol = 6, nrow = 2)\
\
\
\
}